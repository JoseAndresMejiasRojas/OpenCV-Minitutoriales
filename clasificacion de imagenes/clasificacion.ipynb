{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dfceda9-43bc-4e1e-a1a7-f9e36184d0c1",
   "metadata": {},
   "source": [
    "# Clasificación de imágenes usando OpenCV y CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b2234-ef81-423a-93bb-e157ada7b172",
   "metadata": {},
   "source": [
    "## Cargar y explorar datos\n",
    "\n",
    "Primeramente cargamos los datos desde un dataset real que provee keras. Este dataset contiene **imágenes**, representadas de ahora en adelante con una *x* (`x_train`, `x_test`) y sus respectivas **etiquetas**, representadas con una *y* (`y_train`, `y_test`). Note que los 4 resultados que brinda `load_data()` son de tipo ndarray, que es del paquete numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc2d10-043a-4699-a670-96cd13a59b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(type(x_train), type(y_train), type(x_test), type(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bced6a-0a7d-4767-a716-5d1e0aef737a",
   "metadata": {},
   "source": [
    "El primer valor de los 4 arrays son las filas. Por lo tanto, las imágenes y etiquetas de entrenamiento tienen 50000 filas, y las imágenes y etiquetas de prueba tienen 10000. Todas las etiquetas tienen una sola columna indicando la etiqueta a la que pertenece su análogo en *x*. Sin embargo, lo interesante son las dimensiones de las imágenes. Cada fila de *x* tiene una imagen de 32x32 con 3 colores (rojo, verde, azul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8543f7c-2778-4d8b-8820-1917c8bb36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43577f37-49d1-4d5b-ab5a-3da02f81b9d1",
   "metadata": {},
   "source": [
    "## Definición de etiquetas\n",
    "\n",
    "Este dataset en particular contiene las siguientes definidas en `etiquetas_dataset`. Este array lo usaremos más adelante para determinar si nuestro modelo pudo determinar de manera correcta la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ae8f5-e04a-4173-a34a-fc9a007ab1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_dataset = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424e280-d882-4278-bbdb-2d7aef3d68d6",
   "metadata": {},
   "source": [
    "## Explorar una imagen de entrenamiento\n",
    "\n",
    "Exploramos y desplegamos una imagen al azar para entender el contenido de una fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c0bab-77f4-4dc2-ae4c-bb67d6c5d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ejemplo_index = 203\n",
    "x_train[ejemplo_index]\n",
    "\n",
    "print(x_train[ejemplo_index])\n",
    "img = plt.imshow(x_train[ejemplo_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13e9fa-da01-4f83-b2a4-1a50f3f22c61",
   "metadata": {},
   "source": [
    "## Explorar una etiqueta de entrenamiento\n",
    "\n",
    "Es importante tomar en cuenta que las etiquetas están representadas por números. Por lo tanto, siempre tendremos que tomar en cuenta el array definido en `etiquetas_dataset` para identificar la etiqueta respectiva. Esto aplicando tanto para el entrenamiento, la prueba, como la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5faafb-4516-4889-827c-4eebd72bf3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valor 'Y': \", y_train[ejemplo_index])\n",
    "print(\"Palabra 'Y': \", etiquetas_dataset[y_train[ejemplo_index][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04038d-e10e-4b43-9983-25039c34c591",
   "metadata": {},
   "source": [
    "## Covertir las etiquetas en números\n",
    "\n",
    "Para realizar el entrenamiento, el modelo en **y** necesita que cada fila sea un array de 10 elementos (la cantidad de etiquetas). Por lo tanto, ocupamos convetir esas etiquetas a números únicos para cada etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d1039-31ec-437e-a401-a54ecd30d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_numeros = to_categorical(y_train)\n",
    "y_test_numeros = to_categorical(y_test)\n",
    "\n",
    "print(\"'Y' valor ententero: \", y_train_numeros[ejemplo_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3694eb2-b3be-473d-81bb-30103434aa28",
   "metadata": {},
   "source": [
    "## Normalizar las imágenes\n",
    "\n",
    "La otra preparación que necesitamos hacer es normalizar las imágenes *x*. Por lo que los pixels tendrán valores entre 0 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3919625-3b11-4b6b-8240-2b7640f3bd77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "x_train[ejemplo_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd67884-dd4a-4144-8828-36f5a2d3a76d",
   "metadata": {},
   "source": [
    "## Arquitectura para el modelo\n",
    "Definimos una arquitectura para el modelo.\n",
    "\n",
    "- **Primera capa - Conv2D:** Conv2D representa una capa convolucional de dos dimensiones. `32` es el número de filtros requeridos y `(5, 5)` es el tamaño del filtro. El `input_shape` son las dimensiones de cada imagen (32x32 y los 3 colores RGB). La salida de esta capa son *feature maps*, los cuales son mapas con características (*features*) de la imagen.\n",
    "- **Capa pooling:** Es una capa para reducir dimensionalidad de cada *feature map* pero manteniendo información relevante. Esto ayuda reduciendo la complejidad computación de la red neuronal.\n",
    "- **Capa flatten**: Se usa para aplanar. Convierte los *feature map* a una sola dimensión.\n",
    "- **Capa dropout**: Se usa para prevenir el sobreajuste. Es decir, prevenir el sobreentrenamiento.\n",
    "- **Funciones de activación:**\n",
    "    - **ReLu**: Reemplaza todos los pixeles negativos en los *features maps*\n",
    "    - **Softmax**: Se usa para normalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f8799-daa7-46ae-ae03-32f9a807447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "def construir_modelo():\n",
    "    modelo = Sequential()\n",
    "    \n",
    "    # Primera capa\n",
    "    modelo.add(Conv2D(32, (5, 5), activation=\"relu\", input_shape=(32, 32, 3)))\n",
    "\n",
    "    # Capa pooling\n",
    "    modelo.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Capa Conv2D\n",
    "    modelo.add(Conv2D(32, (5, 5), activation=\"relu\"))\n",
    "\n",
    "    # Capa pooling\n",
    "    modelo.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Capa flatten\n",
    "    modelo.add(Flatten())\n",
    "\n",
    "    # Capa ReLu - 1000 neuronas\n",
    "    modelo.add(Dense(1000, activation=\"relu\"))\n",
    "\n",
    "    # Dropout al 50%\n",
    "    modelo.add(Dropout(0.5))\n",
    "\n",
    "    # Capa ReLu - 500 neuronas\n",
    "    modelo.add(Dense(500, activation=\"relu\"))\n",
    "\n",
    "    # Dropout al 50%\n",
    "    modelo.add(Dropout(0.5))\n",
    "\n",
    "    # Capa ReLu - 250 neuronas\n",
    "    modelo.add(Dense(250, activation=\"relu\"))\n",
    "\n",
    "    # Capa Softmax - 10 neuronas\n",
    "    modelo.add(Dense(10, activation=\"softmax\"))\n",
    "    return modelo\n",
    "\n",
    "modelo = construir_modelo()\n",
    "print(modelo.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fbc62-fae7-4f12-aee0-3592d8a74350",
   "metadata": {},
   "source": [
    "## Compilar el modelo\n",
    "\n",
    "Luego de definir la arquitectura, obtenemos el modelo y lo compilamos para empezar a entrenar y predecir. `categorical_crossentropy` compara la distribución de las predicciones con las actuales distribuciones. Usamos el optimizador `adam` por su rendimiento. Además, definimos que queremos visualizar la precisión en las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89934a3f-1ab0-4749-9db8-4ee3f3950fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08f024-136b-4b88-abe9-083ba9f8b369",
   "metadata": {},
   "source": [
    "## Realizamos el entrenamiento\n",
    "\n",
    "Para realizar el entrenamiento, pasamos como parámetro las imágenes de entrenamiento `x_train`, las etiquetas de entrenamiento en números `y_train_enteros`, y una configuración predeterminada.  \n",
    "El `batch_size` se refiere a la cantidad de elementos del conjunto de entrenamiento que se utilizan para entrenar la red en cada iteración. `epochs` es la cantidad de veces que se pasan **todos** los datos por la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f67154-60dc-4993-b7a9-d6440c320704",
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamiento = modelo.fit(x_train, y_train_numeros, batch_size=256, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21cab1-b1d0-4d6f-8b76-d3cbbfaa93e1",
   "metadata": {},
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "Realizamos una evaluación del modelo llamando al método `evaluate`. Note que ocupamos pasar las imágenes de prueba `x_test` y las etiquetas de prueba `y_test_enteros`. En este caso en particular, la precisión ronda alrededor del 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e24d52-e2d8-4e77-8d14-9e51f7bc2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.evaluate(x_test, y_test_numeros)[1]\n",
    "\n",
    "plt.plot(entrenamiento.history[\"accuracy\"])\n",
    "plt.plot(entrenamiento.history[\"val_accuracy\"])\n",
    "plt.title(\"Precision del modelo\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Pasadas del entrenamiento\")\n",
    "plt.legend([\"Train\", \"Val\"], loc=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5c9012-9c7f-419e-841f-56fde1430f93",
   "metadata": {},
   "source": [
    "## Ejemplo real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a2e8f-4985-4406-8faf-f7e5de99190d",
   "metadata": {},
   "source": [
    "### Predicción\n",
    "\n",
    "Para realizar una predicción con una imagen real, necesitamos primeramente cargar la imagen y transformarla obligatoriamente a 32x32. Recuerde que definimos esas dimensiones a la hora de crear y entrenar el modelo. Luego, usamos el método `modelo.predict(np.array([imagen_pequenna]))`. `predict` recibe un array de numpy con la imagen transformada previamente. La predicción retorna un array con un solo elemento también de tipo array, por lo que por comodidad devolvemos el array interno. Dicho \"array interno\" tiene 10 elementos. Cada elemento representa el nivel de confianza de la predicción para cada etiqueta respectivamente en `etiquetas_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e771e3-413e-4ca5-9c64-8a3259608cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def obtener_prediccion(imagen):\n",
    "    # Cargar imagen y transformarla a 32x32\n",
    "    nueva_imagen = cv2.imread(imagen)\n",
    "    nueva_imagen = cv2.cvtColor(nueva_imagen, cv2.COLOR_BGR2RGB)\n",
    "    imagen_pequenna = cv2.resize(nueva_imagen, (32,32))\n",
    "\n",
    "    # Realizar prediccion\n",
    "    prediccion_array = modelo.predict(np.array([imagen_pequenna]))\n",
    "    return prediccion_array[0]\n",
    "\n",
    "prediccion_perrito = obtener_prediccion(\"perrito.jpg\")\n",
    "print(prediccion_perrito)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c19b4-c5dd-4f99-b90f-560aaf00e94d",
   "metadata": {},
   "source": [
    "### Desplegar resultados de la predicción\n",
    "\n",
    "Se define un array `etiquetas_predicciones` de 0 a 9 que representan las etiquetas definidas previamente. Luego, de mayor a menor y tomando en cuenta `prediccion_array`, se ordena `etiquetas_predicciones`. Con este procedimiento podemos obtener las etiquetas con mejor precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b36207-d478-4a99-9504-6a26d972352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_etiquetas_relevantes(prediccion): \n",
    "    etiquetas_predicciones = [0,1,2,3,4,5,6,7,8,9]\n",
    "    # Ordenar el array manualmente de acuerdo a prediccion de mayor a menos\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if prediccion[etiquetas_predicciones[i]] > prediccion[etiquetas_predicciones[j]]:\n",
    "                temp = etiquetas_predicciones[i]\n",
    "                etiquetas_predicciones[i] = etiquetas_predicciones[j]\n",
    "                etiquetas_predicciones[j] = temp\n",
    "    return etiquetas_predicciones\n",
    "\n",
    "def imprimir_5_mejores_predicciones(etiquetas_predicciones):\n",
    "    print(etiquetas_predicciones)\n",
    "    for i in range(5):\n",
    "        print(etiquetas_dataset[etiquetas_predicciones[i]])\n",
    "        \n",
    "etiquetas_predicciones = obtener_etiquetas_relevantes(prediccion_perrito)\n",
    "imprimir_5_mejores_predicciones(etiquetas_predicciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f9a36-d053-497f-981c-199d0943a8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
